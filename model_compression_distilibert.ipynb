{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"gpuType":"A100","authorship_tag":"ABX9TyN2tc/8tef9qRd/N+IUnzku"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["### Shrinking distiliBERT in 3 ways"],"metadata":{"id":"RL5EcOBZNGps"}},{"cell_type":"code","source":["pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VFbIeMI1NQnD","executionInfo":{"status":"ok","timestamp":1687980779056,"user_tz":-180,"elapsed":4292,"user":{"displayName":"Dana Tal","userId":"05782097349471177651"}},"outputId":"27152076-9a0c-43cb-b393-c285f911539b"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"]}]},{"cell_type":"code","source":["pip install neural_compressor"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7bFIGtKeaTqm","executionInfo":{"status":"ok","timestamp":1687980783665,"user_tz":-180,"elapsed":4615,"user":{"displayName":"Dana Tal","userId":"05782097349471177651"}},"outputId":"d42f1e77-c389-4174-c3bb-f2f9d0137a12"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: neural_compressor in /usr/local/lib/python3.10/dist-packages (2.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from neural_compressor) (1.22.4)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from neural_compressor) (1.5.3)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from neural_compressor) (6.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from neural_compressor) (1.2.2)\n","Requirement already satisfied: schema in /usr/local/lib/python3.10/dist-packages (from neural_compressor) (0.7.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from neural_compressor) (9.0.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from neural_compressor) (2.27.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from neural_compressor) (5.9.5)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from neural_compressor) (8.4.0)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from neural_compressor) (4.7.0.72)\n","Requirement already satisfied: prettytable in /usr/local/lib/python3.10/dist-packages (from neural_compressor) (0.7.2)\n","Requirement already satisfied: deprecated>=1.2.13 in /usr/local/lib/python3.10/dist-packages (from neural_compressor) (1.2.14)\n","Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (from neural_compressor) (2.0.6)\n","Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.13->neural_compressor) (1.14.1)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->neural_compressor) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->neural_compressor) (2022.7.1)\n","Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools->neural_compressor) (3.7.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->neural_compressor) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->neural_compressor) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->neural_compressor) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->neural_compressor) (3.4)\n","Requirement already satisfied: contextlib2>=0.5.5 in /usr/local/lib/python3.10/dist-packages (from schema->neural_compressor) (0.6.0.post1)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->neural_compressor) (1.10.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->neural_compressor) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->neural_compressor) (3.1.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->neural_compressor) (1.1.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->neural_compressor) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->neural_compressor) (4.40.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->neural_compressor) (1.4.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->neural_compressor) (23.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->neural_compressor) (3.1.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->neural_compressor) (1.16.0)\n"]}]},{"cell_type":"code","source":["pip install optimum"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6gogdoIaaSrD","executionInfo":{"status":"ok","timestamp":1687980788293,"user_tz":-180,"elapsed":4632,"user":{"displayName":"Dana Tal","userId":"05782097349471177651"}},"outputId":"03253bc6-0b8a-4e59-d3c5-cb2c475e8972"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: optimum in /usr/local/lib/python3.10/dist-packages (1.8.8)\n","Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from optimum) (15.0.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from optimum) (1.11.1)\n","Requirement already satisfied: transformers[sentencepiece]>=4.26.0 in /usr/local/lib/python3.10/dist-packages (from optimum) (4.30.2)\n","Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.10/dist-packages (from optimum) (2.0.1+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from optimum) (0.15.2+cu118)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from optimum) (23.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optimum) (1.22.4)\n","Requirement already satisfied: huggingface-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from optimum) (0.15.1)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from optimum) (2.13.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum) (3.12.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum) (2.27.1)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum) (4.65.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum) (4.6.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->optimum) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->optimum) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->optimum) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.9->optimum) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.9->optimum) (16.0.6)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum) (2022.10.31)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum) (0.3.1)\n","Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum) (0.1.99)\n","Requirement already satisfied: protobuf<=3.20.3 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum) (3.20.3)\n","Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->optimum) (10.0)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->optimum) (9.0.0)\n","Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->optimum) (0.3.6)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->optimum) (1.5.3)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->optimum) (3.2.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->optimum) (0.70.14)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->optimum) (3.8.4)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->optimum) (1.3.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->optimum) (8.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum) (2.0.12)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum) (4.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum) (1.3.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum) (1.3.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.8.0->optimum) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.8.0->optimum) (2023.5.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9->optimum) (2.1.3)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->optimum) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->optimum) (2022.7.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->optimum) (1.16.0)\n"]}]},{"cell_type":"code","source":["! pip install datasets transformers optimum[intel]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W_aVVixeTumR","executionInfo":{"status":"ok","timestamp":1687980792525,"user_tz":-180,"elapsed":4236,"user":{"displayName":"Dana Tal","userId":"05782097349471177651"}},"outputId":"87b127eb-a0d1-4366-e040-c42f1ccbf6f9"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.13.1)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.2)\n","Requirement already satisfied: optimum[intel] in /usr/local/lib/python3.10/dist-packages (1.8.8)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.6)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.2.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.14)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.4)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.15.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n","Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from optimum[intel]) (15.0.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from optimum[intel]) (1.11.1)\n","Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.10/dist-packages (from optimum[intel]) (2.0.1+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from optimum[intel]) (0.15.2+cu118)\n","Requirement already satisfied: optimum-intel in /usr/local/lib/python3.10/dist-packages (from optimum[intel]) (1.9.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.6.3)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.5.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->optimum[intel]) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->optimum[intel]) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->optimum[intel]) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.9->optimum[intel]) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.9->optimum[intel]) (16.0.6)\n","Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.1.99)\n","Requirement already satisfied: protobuf<=3.20.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (3.20.3)\n","Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->optimum[intel]) (10.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from optimum-intel->optimum[intel]) (1.10.1)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from optimum-intel->optimum[intel]) (0.20.3)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->optimum[intel]) (1.3.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->optimum[intel]) (8.4.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->optimum-intel->optimum[intel]) (5.9.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9->optimum[intel]) (2.1.3)\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dwJ6RgcWOkVM","executionInfo":{"status":"ok","timestamp":1687980794355,"user_tz":-180,"elapsed":1837,"user":{"displayName":"Dana Tal","userId":"05782097349471177651"}},"outputId":"7fb97c17-2ad0-4a3c-e9a3-5cb9c3ff28c0"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["from transformers import DistilBertConfig\n","\n","checkpoint_dir = '/content/drive/MyDrive/Colab Notebooks/advanced_deep_learning_group12/distilibert_full/20230628-11-22-26/checkpoint-21000'\n","\n","# Load the model configuration\n","config = DistilBertConfig.from_pretrained(checkpoint_dir)\n","\n","# Print the model configuration\n","#print(config)\n"],"metadata":{"id":"oLj3y5BoNN0I","executionInfo":{"status":"ok","timestamp":1687980796541,"user_tz":-180,"elapsed":2189,"user":{"displayName":"Dana Tal","userId":"05782097349471177651"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["from transformers import DistilBertConfig, DistilBertForSequenceClassification, DistilBertTokenizer\n","import torch\n","from torch.quantization import quantize_dynamic\n","\n","# Load the model from a directory\n","model_path = \"/content/drive/MyDrive/Colab Notebooks/advanced_deep_learning_group12/distilibert_full/20230628-11-22-26/checkpoint-21000\"\n","config = DistilBertConfig.from_pretrained(model_path)\n","model = DistilBertForSequenceClassification.from_pretrained(model_path, config=config)\n"],"metadata":{"id":"_UxlbpFKPM-O","executionInfo":{"status":"ok","timestamp":1687980800236,"user_tz":-180,"elapsed":3698,"user":{"displayName":"Dana Tal","userId":"05782097349471177651"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["#### Pruning the Model"],"metadata":{"id":"t2YNdeAYRx9W"}},{"cell_type":"code","source":["# Method 1: Pruning\n","from transformers import DistilBertConfig, DistilBertForSequenceClassification, DistilBertTokenizer\n","import torch\n","import torch.nn.utils.prune as prune\n","\n","\n","# Prune the model\n","parameters_to_prune = []\n","for name, module in model.named_modules():\n","    if isinstance(module, torch.nn.Linear):\n","        parameters_to_prune.append((module, 'weight'))\n","\n","for layer, parameter_name in parameters_to_prune:\n","    prune.l1_unstructured(layer, name=parameter_name, amount=0.0001)\n","\n","\n","# Save the pruned model\n","pruned_model_directory = \"/content/drive/MyDrive/Colab Notebooks/advanced_deep_learning_group12/pruned_model\"\n","model.save_pretrained(pruned_model_directory)\n"],"metadata":{"id":"ojN7sYl2Prgx","executionInfo":{"status":"ok","timestamp":1687982558059,"user_tz":-180,"elapsed":1975,"user":{"displayName":"Dana Tal","userId":"05782097349471177651"}}},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":["#### Quantized Model"],"metadata":{"id":"RMY3R2hIR1zz"}},{"cell_type":"code","source":["!pip install onnx"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zFQT3j07TQDG","executionInfo":{"status":"ok","timestamp":1687980808775,"user_tz":-180,"elapsed":4112,"user":{"displayName":"Dana Tal","userId":"05782097349471177651"}},"outputId":"63c23959-b18f-4bf3-a23d-61cb2ad20d9b"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: onnx in /usr/local/lib/python3.10/dist-packages (1.14.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnx) (1.22.4)\n","Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.3)\n","Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.10/dist-packages (from onnx) (4.6.3)\n"]}]},{"cell_type":"code","source":["from functools import partial\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer\n","from neural_compressor.config import PostTrainingQuantConfig\n","from optimum.intel import INCQuantizer\n","\n","model_name = \"distilbert-base-uncased\"\n","checkpoint_path = \"/content/drive/MyDrive/Colab Notebooks/advanced_deep_learning_group12/distilibert_full/20230628-11-22-26/checkpoint-21000\"\n","\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint_path)\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","# The directory where the quantized model will be saved\n","save_dir = \"/content/drive/MyDrive/Colab Notebooks/advanced_deep_learning_group12/static_quantization\"\n","\n","def preprocess_function(examples, tokenizer):\n","    return tokenizer(examples[\"sentence\"], padding=\"max_length\", max_length=128, truncation=True)\n","\n","# Load the quantization configuration detailing the quantization we wish to apply\n","quantization_config = PostTrainingQuantConfig(approach=\"static\")\n","quantizer = INCQuantizer.from_pretrained(model)\n","# Generate the calibration dataset needed for the calibration step\n","calibration_dataset = quantizer.get_calibration_dataset(\n","    \"glue\",\n","    dataset_config_name=\"sst2\",\n","    preprocess_function=partial(preprocess_function, tokenizer=tokenizer),\n","    num_samples=100,\n","    dataset_split=\"train\",\n",")\n","quantizer = INCQuantizer.from_pretrained(model)\n","# Apply static quantization and save the resulting model\n","quantizer.quantize(\n","    quantization_config=quantization_config,\n","    calibration_dataset=calibration_dataset,\n","    save_directory=save_dir,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HNJZrGzxS6ja","executionInfo":{"status":"ok","timestamp":1687980825242,"user_tz":-180,"elapsed":16487,"user":{"displayName":"Dana Tal","userId":"05782097349471177651"}},"outputId":"af7adb4f-4440-45a4-80d0-1fcd0d6088a2"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:datasets.builder:Found cached dataset glue (/root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n","WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-59b84a2151c0baf1.arrow\n","WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-f118096abbeebe19.arrow\n","The task could not be automatically inferred and will be set to default. Please provide the task argument with the relevant task from conversational, feature-extraction, fill-mask, text-generation, text2text-generation, text-classification, token-classification, multiple-choice, object-detection, question-answering, image-classification, image-segmentation, masked-im, semantic-segmentation, automatic-speech-recognition, audio-classification, audio-frame-classification, audio-xvector, image-to-text, stable-diffusion, zero-shot-image-classification, zero-shot-object-detection. Detailed error: Cannot infer the task from a local directory yet, please specify the task manually.\n","2023-06-28 19:33:33 [INFO] Start auto tuning.\n","2023-06-28 19:33:33 [INFO] Execute the tuning process due to detect the evaluation function.\n","2023-06-28 19:33:33 [INFO] Adaptor has 4 recipes.\n","2023-06-28 19:33:33 [INFO] 0 recipes specified by user.\n","2023-06-28 19:33:33 [INFO] 3 recipes require future tuning.\n","2023-06-28 19:33:33 [INFO] *** Initialize auto tuning\n","2023-06-28 19:33:33 [INFO] {\n","2023-06-28 19:33:33 [INFO]     'PostTrainingQuantConfig': {\n","2023-06-28 19:33:33 [INFO]         'AccuracyCriterion': {\n","2023-06-28 19:33:33 [INFO]             'criterion': 'relative',\n","2023-06-28 19:33:33 [INFO]             'higher_is_better': True,\n","2023-06-28 19:33:33 [INFO]             'tolerable_loss': 0.01,\n","2023-06-28 19:33:33 [INFO]             'absolute': None,\n","2023-06-28 19:33:33 [INFO]             'keys': <bound method AccuracyCriterion.keys of <neural_compressor.config.AccuracyCriterion object at 0x7fb7d732be20>>,\n","2023-06-28 19:33:33 [INFO]             'relative': 0.01\n","2023-06-28 19:33:33 [INFO]         },\n","2023-06-28 19:33:33 [INFO]         'approach': 'post_training_static_quant',\n","2023-06-28 19:33:33 [INFO]         'backend': 'default',\n","2023-06-28 19:33:33 [INFO]         'calibration_sampling_size': [\n","2023-06-28 19:33:33 [INFO]             100\n","2023-06-28 19:33:33 [INFO]         ],\n","2023-06-28 19:33:33 [INFO]         'device': 'cpu',\n","2023-06-28 19:33:33 [INFO]         'diagnosis': False,\n","2023-06-28 19:33:33 [INFO]         'domain': 'auto',\n","2023-06-28 19:33:33 [INFO]         'example_inputs': None,\n","2023-06-28 19:33:33 [INFO]         'excluded_precisions': [\n","2023-06-28 19:33:33 [INFO]         ],\n","2023-06-28 19:33:33 [INFO]         'framework': 'pytorch_fx',\n","2023-06-28 19:33:33 [INFO]         'inputs': [\n","2023-06-28 19:33:33 [INFO]         ],\n","2023-06-28 19:33:33 [INFO]         'model_name': '',\n","2023-06-28 19:33:33 [INFO]         'op_name_dict': None,\n","2023-06-28 19:33:33 [INFO]         'op_type_dict': None,\n","2023-06-28 19:33:33 [INFO]         'outputs': [\n","2023-06-28 19:33:33 [INFO]         ],\n","2023-06-28 19:33:33 [INFO]         'quant_format': 'default',\n","2023-06-28 19:33:33 [INFO]         'quant_level': 'auto',\n","2023-06-28 19:33:33 [INFO]         'recipes': {\n","2023-06-28 19:33:33 [INFO]             'smooth_quant': False,\n","2023-06-28 19:33:33 [INFO]             'smooth_quant_args': {\n","2023-06-28 19:33:33 [INFO]             },\n","2023-06-28 19:33:33 [INFO]             'fast_bias_correction': False,\n","2023-06-28 19:33:33 [INFO]             'weight_correction': False,\n","2023-06-28 19:33:33 [INFO]             'gemm_to_matmul': True,\n","2023-06-28 19:33:33 [INFO]             'graph_optimization_level': None,\n","2023-06-28 19:33:33 [INFO]             'first_conv_or_matmul_quantization': True,\n","2023-06-28 19:33:33 [INFO]             'last_conv_or_matmul_quantization': True,\n","2023-06-28 19:33:33 [INFO]             'pre_post_process_quantization': True,\n","2023-06-28 19:33:33 [INFO]             'add_qdq_pair_to_weight': False,\n","2023-06-28 19:33:33 [INFO]             'optypes_to_exclude_output_quant': [\n","2023-06-28 19:33:33 [INFO]             ],\n","2023-06-28 19:33:33 [INFO]             'dedicated_qdq_pair': False\n","2023-06-28 19:33:33 [INFO]         },\n","2023-06-28 19:33:33 [INFO]         'reduce_range': None,\n","2023-06-28 19:33:33 [INFO]         'TuningCriterion': {\n","2023-06-28 19:33:33 [INFO]             'max_trials': 100,\n","2023-06-28 19:33:33 [INFO]             'objective': 'performance',\n","2023-06-28 19:33:33 [INFO]             'strategy': 'basic',\n","2023-06-28 19:33:33 [INFO]             'strategy_kwargs': None,\n","2023-06-28 19:33:33 [INFO]             'timeout': 0\n","2023-06-28 19:33:33 [INFO]         },\n","2023-06-28 19:33:33 [INFO]         'use_bf16': True\n","2023-06-28 19:33:33 [INFO]     }\n","2023-06-28 19:33:33 [INFO] }\n","2023-06-28 19:33:33 [WARNING] [Strategy] Please install `mpi4py` correctly if using distributed tuning; otherwise, ignore this warning.\n","2023-06-28 19:33:34 [INFO]  Found 6 blocks\n","2023-06-28 19:33:34 [INFO] Attention Blocks: 6\n","2023-06-28 19:33:34 [INFO] FFN Blocks: 6\n","2023-06-28 19:33:34 [INFO] Pass query framework capability elapsed time: 365.18 ms\n","2023-06-28 19:33:34 [INFO] Get FP32 model baseline.\n","2023-06-28 19:33:34 [INFO] Save tuning history to /content/nc_workspace/2023-06-28_19-33-28/./history.snapshot.\n","2023-06-28 19:33:34 [INFO] FP32 baseline is: [Accuracy: 1.0000, Duration (seconds): 0.0000]\n","2023-06-28 19:33:34 [INFO] Quantize the model with default config.\n","2023-06-28 19:33:34 [INFO] Fx trace of the entire model failed, We will conduct auto quantization\n","/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/observer.py:1348: UserWarning: Please use `is_dynamic` instead of `compute_dtype`.                     `compute_dtype` will be deprecated in a future release                     of PyTorch.\n","  warnings.warn(\n","2023-06-28 19:33:35 [WARNING] Please note that calibration sampling size 100 isn't divisible exactly by batch size 8. So the real sampling size is 104.\n","2023-06-28 19:33:44 [INFO] |*********Mixed Precision Statistics********|\n","2023-06-28 19:33:44 [INFO] +---------------------+-------+------+------+\n","2023-06-28 19:33:44 [INFO] |       Op Type       | Total | INT8 | FP32 |\n","2023-06-28 19:33:44 [INFO] +---------------------+-------+------+------+\n","2023-06-28 19:33:44 [INFO] |      Embedding      |   2   |  2   |  0   |\n","2023-06-28 19:33:44 [INFO] |      LayerNorm      |   13  |  0   |  13  |\n","2023-06-28 19:33:44 [INFO] | quantize_per_tensor |   38  |  38  |  0   |\n","2023-06-28 19:33:44 [INFO] |        Linear       |   38  |  38  |  0   |\n","2023-06-28 19:33:44 [INFO] |      dequantize     |   38  |  38  |  0   |\n","2023-06-28 19:33:44 [INFO] |       Dropout       |   6   |  0   |  6   |\n","2023-06-28 19:33:44 [INFO] +---------------------+-------+------+------+\n","2023-06-28 19:33:44 [INFO] Pass quantize model elapsed time: 10363.17 ms\n","2023-06-28 19:33:44 [INFO] Tune 1 result is: [Accuracy (int8|fp32): 1.0000|1.0000, Duration (seconds) (int8|fp32): 0.0000|0.0000], Best tune result is: [Accuracy: 1.0000, Duration (seconds): 0.0000]\n","2023-06-28 19:33:44 [INFO] |**********************Tune Result Statistics**********************|\n","2023-06-28 19:33:44 [INFO] +--------------------+----------+---------------+------------------+\n","2023-06-28 19:33:44 [INFO] |     Info Type      | Baseline | Tune 1 result | Best tune result |\n","2023-06-28 19:33:44 [INFO] +--------------------+----------+---------------+------------------+\n","2023-06-28 19:33:44 [INFO] |      Accuracy      | 1.0000   |    1.0000     |     1.0000       |\n","2023-06-28 19:33:44 [INFO] | Duration (seconds) | 0.0000   |    0.0000     |     0.0000       |\n","2023-06-28 19:33:44 [INFO] +--------------------+----------+---------------+------------------+\n","2023-06-28 19:33:44 [INFO] Save tuning history to /content/nc_workspace/2023-06-28_19-33-28/./history.snapshot.\n","2023-06-28 19:33:44 [INFO] Specified timeout or max trials is reached! Found a quantized model which meet accuracy goal. Exit.\n","2023-06-28 19:33:44 [INFO] Save deploy yaml to /content/nc_workspace/2023-06-28_19-33-28/deploy.yaml\n","Model weights saved to /content/drive/MyDrive/Colab Notebooks/advanced_deep_learning_group12/static_quantization/pytorch_model.bin\n","Configuration saved in /content/drive/MyDrive/Colab Notebooks/advanced_deep_learning_group12/static_quantization/inc_config.json\n"]}]},{"cell_type":"markdown","source":["#### Knowledge Distiliation"],"metadata":{"id":"KB3ui9SoUa2g"}},{"cell_type":"code","source":["import pandas as pd\n","\n","data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/advanced_deep_learning_group12/processed_data.csv')\n","data.dropna(inplace = True)\n","\n","# 1) Choose 30% of the data (balanced by categories)\n","undersampled_data = data.groupby('headline_main_category').apply(lambda x: x.sample(frac=0.0005, random_state=42)).reset_index(drop=True)\n","\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","\n","# 1) Split the data into train and test (balanced by categories)\n","train_data, test_data = train_test_split(undersampled_data, test_size=0.2, stratify=undersampled_data['headline_main_category'], random_state=42)\n","\n","# 2) Encode the 'headline_main_category' column as integer labels\n","label_encoder = LabelEncoder()\n","train_data['encoded_category'] = label_encoder.fit_transform(train_data['headline_main_category'])\n","test_data['encoded_category'] = label_encoder.transform(test_data['headline_main_category'])\n","\n","from transformers import DistilBertTokenizer, AlbertTokenizer\n","\n","# Initialize the tokenizers\n","distilbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","\n","# 4) Tokenize the 'headline_text_processed' column\n","train_distilbert_tokenized = train_data['headline_text_processed'].apply(distilbert_tokenizer.encode)\n","test_distilbert_tokenized = test_data['headline_text_processed'].apply(distilbert_tokenizer.encode)\n","\n","\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","# Get the maximum length of tokenized inputs for DistilBERT\n","max_length_distilbert = max(len(tokens) for tokens in train_distilbert_tokenized)\n","max_length_distilbert += 2  # Add extra buffer\n","\n","# Pad the tokenized inputs\n","train_distilbert_padded = pad_sequences(train_distilbert_tokenized, maxlen=max_length_distilbert, padding='post', truncating='post')\n","test_distilbert_padded = pad_sequences(test_distilbert_tokenized, maxlen=max_length_distilbert, padding='post', truncating='post')\n","\n","from torch.utils.data import DataLoader, Dataset\n","\n","# Define a custom dataset class for PyTorch\n","class Dataset(Dataset):\n","    def __init__(self, tokenized_inputs, labels):\n","      self.tokenized_inputs = tokenized_inputs  # Tokenized and padded input sequences\n","      self.labels = labels  # Encoded labels (categories)\n","\n","    def __len__(self):\n","      return len(self.labels)  # Return the number of samples\n","\n","    def __getitem__(self, idx):\n","      return {'input_ids': torch.tensor(self.tokenized_inputs[idx], dtype=torch.long),  # Convert input sequence to tensor\n","              'labels': torch.tensor(self.labels[idx], dtype=torch.long),  # Convert label to tensor\n","              }\n","\n","distilibert_train_dataset = Dataset(train_distilbert_padded, train_data['encoded_category'].values)\n","distilibert_test_dataset = Dataset(test_distilbert_padded, test_data['encoded_category'].values)"],"metadata":{"id":"AQd9_3aOb5NF","executionInfo":{"status":"ok","timestamp":1687980832415,"user_tz":-180,"elapsed":7194,"user":{"displayName":"Dana Tal","userId":"05782097349471177651"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoModelForSequenceClassification, TrainingArguments\n","from optimum.intel import INCTrainer#, DistillationConfig\n","from neural_compressor import DistillationConfig\n","from transformers import DistilBertForSequenceClassification, DistilBertTokenizer, Trainer, TrainingArguments\n","import datetime\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","from torch.utils.data import DataLoader, Dataset\n","from transformers import DefaultFlowCallback\n","from transformers import EarlyStoppingCallback\n","from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n","\n","teacher_model_path = 'distilbert-base-uncased'\n","checkpoint_path = \"/content/drive/MyDrive/Colab Notebooks/advanced_deep_learning_group12/distilibert_full/20230628-11-22-26/checkpoint-21000\"  # Path to the student model checkpoint\n","num_classes = len(train_data['encoded_category'].unique())\n","\n","\n","# Load the teacher model\n","teacher_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels = num_classes)\n","\n","# Load the student model\n","student_model = AutoModelForSequenceClassification.from_pretrained(checkpoint_path)\n","\n","# Define the distillation configuration with the teacher model\n","distillation_config = DistillationConfig(teacher_model=teacher_model)\n","\n","# Define the training arguments\n","training_args = TrainingArguments(\n","    output_dir=\"/content/drive/MyDrive/Colab Notebooks/advanced_deep_learning_group12/knowledge_distillation\",  # Directory to save the model and logs\n","    num_train_epochs=1.0,\n","    do_train=True,\n","    do_eval=False,\n",")\n","\n","def compute_metrics(pred):\n","    labels = pred.label_ids\n","    preds = pred.predictions.argmax(-1)\n","    accuracy = accuracy_score(labels, preds)\n","    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n","\n","    return {\n","        'accuracy': accuracy,\n","        'precision': precision,\n","        'recall': recall,\n","        'f1': f1\n","    }\n","\n","\n","# Define the INCTrainer with distillation\n","trainer = INCTrainer(\n","    model=student_model,\n","    distillation_config=distillation_config,\n","    args=training_args,\n","    train_dataset=distilibert_train_dataset,  # Replace with your training dataset\n","    eval_dataset=distilibert_test_dataset,  # Replace with your evaluation dataset\n","    compute_metrics=compute_metrics,  # Replace with your metric function\n",")\n","\n","# Train the student model with knowledge distillation\n","train_result = trainer.train()\n","metrics = trainer.evaluate()\n","trainer.save_model()\n","\n","# Load the trained student model\n","#model = AutoModelForSequenceClassification.from_pretrained(\"/path/to/save_dir\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":567},"id":"acA2zxngVuBI","executionInfo":{"status":"ok","timestamp":1687980883287,"user_tz":-180,"elapsed":9046,"user":{"displayName":"Dana Tal","userId":"05782097349471177651"}},"outputId":"8ccc589e-b535-4be0-b138-0a8fa2cd2588"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","2023-06-28 19:34:37 [WARNING] Force convert framework model to neural_compressor model.\n","2023-06-28 19:34:37 [INFO] student_targets_loss: CE, 0.5\n","2023-06-28 19:34:37 [INFO] teacher_student_loss: CE, 0.5\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 644\n","  Num Epochs = 1\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 81\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='81' max='81' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [81/81 00:03, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","2023-06-28 19:34:42 [INFO] Training finished!\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [21/21 00:00]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/advanced_deep_learning_group12/knowledge_distillation\n","Configuration saved in /content/drive/MyDrive/Colab Notebooks/advanced_deep_learning_group12/knowledge_distillation/inc_config.json\n","Model weights saved in /content/drive/MyDrive/Colab Notebooks/advanced_deep_learning_group12/knowledge_distillation/pytorch_model.bin\n"]}]},{"cell_type":"markdown","source":["### Evaluating the 3 compressed Models"],"metadata":{"id":"hQr2vqYjg05P"}},{"cell_type":"code","source":["import torch\n","\n","# Check if a GPU is available, and if not, use CPU\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Alternatively, you can specify a specific GPU by its index\n","# device = torch.device('cuda:0')  # Use the first GPU\n","\n","# Print the device\n","print(f\"Device: {device}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"72NCOldmiTT9","executionInfo":{"status":"ok","timestamp":1687981474224,"user_tz":-180,"elapsed":6,"user":{"displayName":"Dana Tal","userId":"05782097349471177651"}},"outputId":"1298aa19-8172-488d-8075-67a088cc5825"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Device: cuda\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/advanced_deep_learning_group12/processed_data.csv')\n","data.dropna(inplace = True)\n","\n","# 1) Choose 30% of the data (balanced by categories)\n","test_data = data.groupby('headline_main_category').apply(lambda x: x.sample(frac=0.0005, random_state=103)).reset_index(drop=True)\n","\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","\n","# 2) Encode the 'headline_main_category' column as integer labels\n","label_encoder = LabelEncoder()\n","test_data['encoded_category'] = label_encoder.fit_transform(test_data['headline_main_category'])\n","\n","from transformers import DistilBertTokenizer, AlbertTokenizer\n","\n","# Initialize the tokenizers\n","distilbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","\n","# 4) Tokenize the 'headline_text_processed' column\n","test_distilbert_tokenized = test_data['headline_text_processed'].apply(distilbert_tokenizer.encode)\n","\n","\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","# Get the maximum length of tokenized inputs for DistilBERT\n","max_length_distilbert = max(len(tokens) for tokens in train_distilbert_tokenized)\n","max_length_distilbert += 2  # Add extra buffer\n","\n","# Pad the tokenized inputs\n","eval_distilbert_padded = pad_sequences(test_distilbert_tokenized, maxlen=max_length_distilbert, padding='post', truncating='post')\n"],"metadata":{"id":"s33z7E9UhPas","executionInfo":{"status":"ok","timestamp":1687981390260,"user_tz":-180,"elapsed":7333,"user":{"displayName":"Dana Tal","userId":"05782097349471177651"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["import torch\n","\n","# Load the compressed models\n","model1 = AutoModelForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/Colab Notebooks/advanced_deep_learning_group12/pruned_model\")\n","model2 = AutoModelForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/Colab Notebooks/advanced_deep_learning_group12/static_quantization\")\n","model3 = AutoModelForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/Colab Notebooks/advanced_deep_learning_group12/knowledge_distillation\")\n","\n","# Move the model to the device\n","model1 = model1.to(device)\n","model2 = model2.to(device)\n","model3 = model3.to(device)\n","\n","# Convert eval_distilbert_padded to a PyTorch tensor\n","eval_distilbert_padded = torch.tensor(eval_distilbert_padded, dtype=torch.long).to(device)\n","test_data_labels = test_data['encoded_category'].values\n","eval_dataset = Dataset(eval_distilbert_padded, test_data_labels)\n","\n","# Define a dataloader for batch processing\n","eval_dataloader = DataLoader(eval_dataset, batch_size=16)\n","\n","# Define a function to calculate evaluation metrics\n","def calculate_metrics(pred_labels, true_labels):\n","    accuracy = accuracy_score(true_labels, pred_labels)\n","    precision = precision_score(true_labels, pred_labels, average='weighted')\n","    recall = recall_score(true_labels, pred_labels, average='weighted')\n","    f1 = f1_score(true_labels, pred_labels, average='weighted')\n","    return accuracy, precision, recall, f1\n","\n","# Evaluate model 1\n","model1.eval()\n","model1_predictions = []\n","true_labels = []\n","for batch in eval_dataloader:\n","    input_ids = batch['input_ids'].to(device)\n","    with torch.no_grad():\n","        logits = model1(input_ids)[0]\n","        predictions = logits.argmax(dim=1)\n","    model1_predictions.extend(predictions.tolist())\n","    true_labels.extend(batch['labels'].tolist())\n","\n","model1_accuracy, model1_precision, model1_recall, model1_f1 = calculate_metrics(model1_predictions, true_labels)\n","\n","# Evaluate model 2\n","model2.eval()\n","model2_predictions = []\n","true_labels = []\n","for batch in eval_dataloader:\n","    input_ids = batch['input_ids'].to(device)\n","    with torch.no_grad():\n","        logits = model2(input_ids)[0]\n","        predictions = logits.argmax(dim=1)\n","    model2_predictions.extend(predictions.tolist())\n","    true_labels.extend(batch['labels'].tolist())\n","\n","model2_accuracy, model2_precision, model2_recall, model2_f1 = calculate_metrics(model2_predictions, true_labels)\n","\n","\n","# Evaluate model 3\n","model3.eval()\n","model3_predictions = []\n","true_labels = []\n","for batch in eval_dataloader:\n","    input_ids = batch['input_ids'].to(device)\n","    with torch.no_grad():\n","        logits = model3(input_ids)[0]\n","        predictions = logits.argmax(dim=1)\n","    model3_predictions.extend(predictions.tolist())\n","    true_labels.extend(batch['labels'].tolist())\n","\n","model3_accuracy, model3_precision, model3_recall, model3_f1 = calculate_metrics(model3_predictions, true_labels)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sy30nDa0g4hM","executionInfo":{"status":"ok","timestamp":1687982572338,"user_tz":-180,"elapsed":4332,"user":{"displayName":"Dana Tal","userId":"05782097349471177651"}},"outputId":"2c09ed1e-4221-4d4e-8b31-5b094956b1d2"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at /content/drive/MyDrive/Colab Notebooks/advanced_deep_learning_group12/pruned_model were not used when initializing DistilBertForSequenceClassification: ['distilbert.transformer.layer.2.ffn.lin2.weight_orig', 'distilbert.transformer.layer.4.ffn.lin2.weight_mask', 'distilbert.transformer.layer.3.attention.k_lin.weight_orig', 'distilbert.transformer.layer.3.attention.v_lin.weight_orig', 'distilbert.transformer.layer.0.attention.q_lin.weight_orig', 'distilbert.transformer.layer.1.attention.out_lin.weight_mask', 'distilbert.transformer.layer.2.attention.v_lin.weight_mask', 'distilbert.transformer.layer.1.attention.q_lin.weight_orig', 'distilbert.transformer.layer.0.ffn.lin2.weight_mask', 'pre_classifier.weight_orig', 'distilbert.transformer.layer.3.attention.v_lin.weight_mask', 'distilbert.transformer.layer.0.ffn.lin2.weight_orig', 'distilbert.transformer.layer.4.attention.out_lin.weight_orig', 'distilbert.transformer.layer.2.ffn.lin1.weight_mask', 'distilbert.transformer.layer.4.attention.v_lin.weight_orig', 'distilbert.transformer.layer.1.attention.k_lin.weight_mask', 'distilbert.transformer.layer.1.attention.q_lin.weight_mask', 'distilbert.transformer.layer.2.attention.v_lin.weight_orig', 'distilbert.transformer.layer.0.attention.v_lin.weight_mask', 'distilbert.transformer.layer.5.ffn.lin1.weight_orig', 'distilbert.transformer.layer.4.attention.k_lin.weight_orig', 'distilbert.transformer.layer.1.attention.v_lin.weight_orig', 'distilbert.transformer.layer.2.attention.q_lin.weight_mask', 'distilbert.transformer.layer.0.attention.k_lin.weight_orig', 'distilbert.transformer.layer.1.ffn.lin2.weight_mask', 'distilbert.transformer.layer.3.ffn.lin2.weight_mask', 'distilbert.transformer.layer.5.ffn.lin2.weight_mask', 'pre_classifier.weight_mask', 'distilbert.transformer.layer.1.ffn.lin1.weight_orig', 'distilbert.transformer.layer.5.attention.v_lin.weight_orig', 'distilbert.transformer.layer.4.ffn.lin2.weight_orig', 'distilbert.transformer.layer.4.attention.q_lin.weight_orig', 'distilbert.transformer.layer.0.attention.v_lin.weight_orig', 'distilbert.transformer.layer.3.ffn.lin2.weight_orig', 'distilbert.transformer.layer.3.attention.q_lin.weight_mask', 'distilbert.transformer.layer.4.ffn.lin1.weight_orig', 'distilbert.transformer.layer.4.ffn.lin1.weight_mask', 'distilbert.transformer.layer.0.ffn.lin1.weight_orig', 'distilbert.transformer.layer.2.attention.out_lin.weight_mask', 'distilbert.transformer.layer.2.attention.q_lin.weight_orig', 'distilbert.transformer.layer.3.attention.k_lin.weight_mask', 'distilbert.transformer.layer.4.attention.k_lin.weight_mask', 'distilbert.transformer.layer.5.attention.q_lin.weight_mask', 'distilbert.transformer.layer.1.ffn.lin2.weight_orig', 'distilbert.transformer.layer.2.ffn.lin1.weight_orig', 'distilbert.transformer.layer.3.ffn.lin1.weight_mask', 'distilbert.transformer.layer.5.attention.out_lin.weight_orig', 'distilbert.transformer.layer.1.attention.out_lin.weight_orig', 'distilbert.transformer.layer.2.attention.k_lin.weight_orig', 'distilbert.transformer.layer.0.attention.out_lin.weight_mask', 'distilbert.transformer.layer.1.attention.v_lin.weight_mask', 'distilbert.transformer.layer.3.attention.q_lin.weight_orig', 'distilbert.transformer.layer.5.ffn.lin2.weight_orig', 'distilbert.transformer.layer.3.attention.out_lin.weight_mask', 'distilbert.transformer.layer.2.attention.out_lin.weight_orig', 'distilbert.transformer.layer.0.ffn.lin1.weight_mask', 'distilbert.transformer.layer.5.attention.q_lin.weight_orig', 'classifier.weight_orig', 'distilbert.transformer.layer.0.attention.out_lin.weight_orig', 'distilbert.transformer.layer.3.ffn.lin1.weight_orig', 'distilbert.transformer.layer.5.attention.k_lin.weight_mask', 'distilbert.transformer.layer.5.attention.out_lin.weight_mask', 'distilbert.transformer.layer.5.attention.v_lin.weight_mask', 'distilbert.transformer.layer.0.attention.q_lin.weight_mask', 'distilbert.transformer.layer.4.attention.out_lin.weight_mask', 'distilbert.transformer.layer.2.attention.k_lin.weight_mask', 'distilbert.transformer.layer.1.ffn.lin1.weight_mask', 'distilbert.transformer.layer.4.attention.v_lin.weight_mask', 'distilbert.transformer.layer.1.attention.k_lin.weight_orig', 'distilbert.transformer.layer.5.ffn.lin1.weight_mask', 'distilbert.transformer.layer.5.attention.k_lin.weight_orig', 'distilbert.transformer.layer.3.attention.out_lin.weight_orig', 'classifier.weight_mask', 'distilbert.transformer.layer.4.attention.q_lin.weight_mask', 'distilbert.transformer.layer.0.attention.k_lin.weight_mask', 'distilbert.transformer.layer.2.ffn.lin2.weight_mask']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/Colab Notebooks/advanced_deep_learning_group12/pruned_model and are newly initialized: ['distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'classifier.weight', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'pre_classifier.weight', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.3.ffn.lin2.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of the model checkpoint at /content/drive/MyDrive/Colab Notebooks/advanced_deep_learning_group12/static_quantization were not used when initializing DistilBertForSequenceClassification: ['distilbert.transformer.layer.3.attention.k_lin.module._packed_params._packed_params', 'distilbert.transformer.layer.1.attention.v_lin.module_input_zero_point_0', 'distilbert.transformer.layer.2.output_layer_norm.module.weight', 'distilbert.transformer.layer.1.ffn.lin1_input_scale_0', 'distilbert.transformer.layer.4.attention.out_lin.module._packed_params.dtype', 'distilbert.transformer.layer.3.attention.q_lin.module_input_scale_0', 'distilbert.transformer.layer.5.ffn.lin2._packed_params._packed_params', 'classifier.module._packed_params.dtype', 'distilbert.transformer.layer.3.attention.q_lin.module_input_zero_point_0', 'distilbert.transformer.layer.3.attention.v_lin.module._packed_params._packed_params', 'distilbert.transformer.layer.1.ffn.lin2._packed_params._packed_params', 'distilbert.transformer.layer.4.attention.out_lin.module_input_scale_0', 'distilbert.transformer.layer.3.attention.k_lin.module.scale', 'distilbert.transformer.layer.3.attention.k_lin.module.zero_point', 'distilbert.transformer.layer.0.ffn.lin1_input_zero_point_0', 'distilbert.transformer.layer.3.ffn.lin2.zero_point', 'distilbert.transformer.layer.0.ffn.lin1.scale', 'distilbert.transformer.layer.0.attention.k_lin.module._packed_params.dtype', 'distilbert.transformer.layer.5.ffn.lin1_input_zero_point_0', 'distilbert.transformer.layer.0.attention.v_lin.module._packed_params.dtype', 'distilbert.transformer.layer.5.output_layer_norm.module.bias', 'distilbert.transformer.layer.4.attention.out_lin.module_input_zero_point_0', 'distilbert.transformer.layer.1.attention.k_lin.module._packed_params._packed_params', 'distilbert.transformer.layer.1.attention.v_lin.module.zero_point', 'distilbert.transformer.layer.0.attention.out_lin.module._packed_params.dtype', 'distilbert.transformer.layer.5.attention.out_lin.module_input_scale_0', 'distilbert.transformer.layer.2.attention.v_lin.module.zero_point', 'distilbert.transformer.layer.5.ffn.lin2.zero_point', 'distilbert.transformer.layer.0.output_layer_norm.module.weight', 'distilbert.transformer.layer.2.ffn.lin2._packed_params.dtype', 'distilbert.transformer.layer.2.attention.out_lin.module.scale', 'classifier.module_input_zero_point_0', 'distilbert.transformer.layer.4.attention.q_lin.module_input_scale_0', 'distilbert.transformer.layer.1.attention.k_lin.module_input_zero_point_0', 'distilbert.embeddings.position_embeddings.module._packed_params.dtype', 'distilbert.transformer.layer.0.attention.k_lin.module.zero_point', 'distilbert.transformer.layer.2.ffn.lin1.scale', 'distilbert.transformer.layer.0.attention.q_lin.module.zero_point', 'distilbert.transformer.layer.0.attention.k_lin.module_input_scale_0', 'distilbert.transformer.layer.4.ffn.lin1_input_scale_0', 'distilbert.transformer.layer.4.attention.q_lin.module.zero_point', 'distilbert.transformer.layer.5.ffn.lin2_input_scale_0', 'distilbert.transformer.layer.3.ffn.lin2._packed_params._packed_params', 'distilbert.transformer.layer.3.attention.out_lin.module.zero_point', 'distilbert.transformer.layer.5.attention.v_lin.module._packed_params.dtype', 'distilbert.transformer.layer.5.ffn.lin1.zero_point', 'classifier.module_input_scale_0', 'distilbert.transformer.layer.4.attention.out_lin.module.zero_point', 'distilbert.transformer.layer.4.attention.v_lin.module.scale', 'distilbert.transformer.layer.5.attention.v_lin.module.scale', 'distilbert.transformer.layer.5.ffn.lin2._packed_params.dtype', 'distilbert.transformer.layer.2.attention.q_lin.module.scale', 'distilbert.transformer.layer.0.attention.v_lin.module_input_zero_point_0', 'distilbert.transformer.layer.4.sa_layer_norm.module.bias', 'distilbert.transformer.layer.4.ffn.lin1._packed_params.dtype', 'distilbert.transformer.layer.5.attention.k_lin.module._packed_params._packed_params', 'distilbert.transformer.layer.2.sa_layer_norm.module.bias', 'distilbert.transformer.layer.4.sa_layer_norm.module.weight', 'pre_classifier.module.zero_point', 'distilbert.transformer.layer.0.attention.out_lin.module.zero_point', 'pre_classifier.module_input_scale_0', 'distilbert.transformer.layer.1.output_layer_norm.module.bias', 'distilbert.transformer.layer.2.ffn.lin1_input_zero_point_0', 'distilbert.transformer.layer.0.attention.k_lin.module.scale', 'distilbert.transformer.layer.1.attention.v_lin.module._packed_params.dtype', 'distilbert.transformer.layer.3.ffn.lin1._packed_params._packed_params', 'distilbert.transformer.layer.5.ffn.lin1_input_scale_0', 'distilbert.transformer.layer.1.ffn.lin1._packed_params._packed_params', 'distilbert.transformer.layer.3.attention.out_lin.module._packed_params.dtype', 'distilbert.transformer.layer.0.attention.k_lin.module._packed_params._packed_params', 'distilbert.transformer.layer.4.ffn.lin2._packed_params.dtype', 'distilbert.transformer.layer.4.attention.q_lin.module._packed_params._packed_params', 'distilbert.transformer.layer.5.ffn.lin2.scale', 'distilbert.transformer.layer.0.attention.q_lin.module._packed_params._packed_params', 'distilbert.transformer.layer.1.ffn.lin2_input_zero_point_0', 'distilbert.transformer.layer.5.ffn.lin2_input_zero_point_0', 'distilbert.transformer.layer.2.ffn.lin2._packed_params._packed_params', 'distilbert.transformer.layer.0.ffn.lin2_input_scale_0', 'distilbert.transformer.layer.5.ffn.lin1._packed_params._packed_params', 'distilbert.transformer.layer.5.attention.out_lin.module._packed_params.dtype', 'distilbert.transformer.layer.3.ffn.lin2.scale', 'distilbert.transformer.layer.3.ffn.lin2_input_scale_0', 'distilbert.transformer.layer.3.attention.q_lin.module.zero_point', 'distilbert.transformer.layer.1.ffn.lin2._packed_params.dtype', 'distilbert.transformer.layer.0.ffn.lin1_input_scale_0', 'distilbert.transformer.layer.4.attention.k_lin.module.scale', 'distilbert.transformer.layer.0.attention.out_lin.module._packed_params._packed_params', 'distilbert.transformer.layer.5.attention.k_lin.module.scale', 'distilbert.transformer.layer.2.attention.out_lin.module_input_zero_point_0', 'distilbert.embeddings.position_embeddings.module._packed_params._packed_weight', 'distilbert.transformer.layer.5.attention.out_lin.module_input_zero_point_0', 'distilbert.transformer.layer.5.sa_layer_norm.module.weight', 'distilbert.transformer.layer.1.ffn.lin2.scale', 'distilbert.transformer.layer.1.ffn.lin1_input_zero_point_0', 'distilbert.transformer.layer.2.attention.v_lin.module._packed_params.dtype', 'distilbert.transformer.layer.1.attention.out_lin.module.zero_point', 'distilbert.transformer.layer.4.attention.k_lin.module.zero_point', 'distilbert.transformer.layer.0.ffn.lin1.zero_point', 'distilbert.transformer.layer.2.attention.q_lin.module_input_zero_point_0', 'distilbert.transformer.layer.3.ffn.lin2._packed_params.dtype', 'distilbert.transformer.layer.0.ffn.lin2._packed_params.dtype', 'distilbert.transformer.layer.4.ffn.lin1.scale', 'distilbert.transformer.layer.5.attention.q_lin.module._packed_params.dtype', 'distilbert.transformer.layer.5.attention.k_lin.module._packed_params.dtype', 'distilbert.transformer.layer.5.attention.v_lin.module.zero_point', 'distilbert.transformer.layer.0.attention.q_lin.module_input_zero_point_0', 'distilbert.transformer.layer.4.ffn.lin2._packed_params._packed_params', 'distilbert.transformer.layer.0.attention.v_lin.module._packed_params._packed_params', 'distilbert.transformer.layer.2.output_layer_norm.module.bias', 'distilbert.transformer.layer.4.attention.k_lin.module_input_scale_0', 'distilbert.transformer.layer.5.attention.k_lin.module.zero_point', 'distilbert.transformer.layer.0.ffn.lin2.zero_point', 'distilbert.transformer.layer.2.attention.q_lin.module._packed_params.dtype', 'distilbert.transformer.layer.3.ffn.lin1.zero_point', 'distilbert.transformer.layer.2.sa_layer_norm.module.weight', 'distilbert.transformer.layer.0.attention.q_lin.module._packed_params.dtype', 'distilbert.transformer.layer.3.attention.q_lin.module._packed_params._packed_params', 'distilbert.transformer.layer.3.output_layer_norm.module.bias', 'distilbert.transformer.layer.3.attention.out_lin.module.scale', 'distilbert.transformer.layer.1.attention.v_lin.module_input_scale_0', 'distilbert.transformer.layer.2.attention.v_lin.module.scale', 'distilbert.transformer.layer.3.attention.v_lin.module.scale', 'distilbert.transformer.layer.1.attention.q_lin.module.scale', 'distilbert.transformer.layer.1.ffn.lin2.zero_point', 'distilbert.transformer.layer.4.attention.k_lin.module._packed_params.dtype', 'distilbert.transformer.layer.5.attention.q_lin.module._packed_params._packed_params', 'distilbert.transformer.layer.4.attention.k_lin.module._packed_params._packed_params', 'distilbert.transformer.layer.0.attention.v_lin.module.zero_point', 'distilbert.transformer.layer.2.attention.out_lin.module.zero_point', 'distilbert.transformer.layer.0.ffn.lin1._packed_params.dtype', 'distilbert.transformer.layer.3.ffn.lin1._packed_params.dtype', 'distilbert.transformer.layer.1.attention.q_lin.module_input_scale_0', 'distilbert.transformer.layer.1.attention.out_lin.module.scale', 'distilbert.transformer.layer.5.attention.k_lin.module_input_scale_0', 'distilbert.transformer.layer.1.output_layer_norm.module.weight', 'distilbert.transformer.layer.2.ffn.lin1.zero_point', 'distilbert.transformer.layer.2.attention.v_lin.module._packed_params._packed_params', 'distilbert.transformer.layer.5.attention.out_lin.module.zero_point', 'distilbert.transformer.layer.2.attention.q_lin.module_input_scale_0', 'distilbert.transformer.layer.2.attention.q_lin.module._packed_params._packed_params', 'distilbert.transformer.layer.1.attention.k_lin.module._packed_params.dtype', 'classifier.module.zero_point', 'pre_classifier.module._packed_params._packed_params', 'pre_classifier.module.scale', 'distilbert.transformer.layer.0.attention.v_lin.module_input_scale_0', 'distilbert.transformer.layer.3.attention.out_lin.module_input_zero_point_0', 'distilbert.embeddings.word_embeddings.module._packed_params._packed_weight', 'distilbert.transformer.layer.3.attention.k_lin.module_input_scale_0', 'distilbert.transformer.layer.4.ffn.lin1_input_zero_point_0', 'pre_classifier.module._packed_params.dtype', 'distilbert.transformer.layer.1.attention.out_lin.module_input_scale_0', 'distilbert.transformer.layer.3.attention.v_lin.module_input_zero_point_0', 'best_configure', 'distilbert.transformer.layer.5.attention.q_lin.module_input_scale_0', 'distilbert.transformer.layer.4.attention.v_lin.module_input_scale_0', 'distilbert.transformer.layer.4.ffn.lin1._packed_params._packed_params', 'distilbert.transformer.layer.0.ffn.lin2._packed_params._packed_params', 'distilbert.transformer.layer.5.attention.out_lin.module.scale', 'distilbert.transformer.layer.2.attention.out_lin.module_input_scale_0', 'distilbert.transformer.layer.3.attention.v_lin.module_input_scale_0', 'distilbert.transformer.layer.2.attention.k_lin.module.scale', 'distilbert.transformer.layer.2.attention.out_lin.module._packed_params._packed_params', 'distilbert.transformer.layer.2.ffn.lin2_input_scale_0', 'distilbert.transformer.layer.2.ffn.lin2_input_zero_point_0', 'distilbert.transformer.layer.1.attention.k_lin.module.zero_point', 'distilbert.transformer.layer.0.attention.out_lin.module_input_zero_point_0', 'distilbert.transformer.layer.1.ffn.lin2_input_scale_0', 'distilbert.transformer.layer.4.attention.v_lin.module.zero_point', 'distilbert.transformer.layer.4.attention.k_lin.module_input_zero_point_0', 'distilbert.transformer.layer.1.attention.q_lin.module_input_zero_point_0', 'distilbert.transformer.layer.1.attention.out_lin.module._packed_params._packed_params', 'distilbert.transformer.layer.1.attention.out_lin.module._packed_params.dtype', 'distilbert.transformer.layer.1.ffn.lin1.zero_point', 'distilbert.transformer.layer.4.attention.q_lin.module_input_zero_point_0', 'classifier.module.scale', 'distilbert.transformer.layer.5.attention.q_lin.module_input_zero_point_0', 'distilbert.transformer.layer.1.ffn.lin1._packed_params.dtype', 'distilbert.transformer.layer.5.ffn.lin1.scale', 'distilbert.transformer.layer.0.output_layer_norm.module.bias', 'distilbert.transformer.layer.2.attention.v_lin.module_input_scale_0', 'distilbert.transformer.layer.2.attention.k_lin.module_input_scale_0', 'distilbert.transformer.layer.0.attention.k_lin.module_input_zero_point_0', 'distilbert.transformer.layer.0.attention.v_lin.module.scale', 'distilbert.transformer.layer.3.attention.k_lin.module._packed_params.dtype', 'distilbert.transformer.layer.4.ffn.lin2_input_scale_0', 'distilbert.transformer.layer.3.ffn.lin1_input_scale_0', 'distilbert.transformer.layer.4.ffn.lin2.scale', 'distilbert.transformer.layer.4.ffn.lin2_input_zero_point_0', 'distilbert.transformer.layer.2.ffn.lin2.zero_point', 'distilbert.transformer.layer.3.attention.out_lin.module._packed_params._packed_params', 'distilbert.transformer.layer.0.sa_layer_norm.module.bias', 'distilbert.transformer.layer.1.attention.k_lin.module_input_scale_0', 'distilbert.transformer.layer.3.attention.k_lin.module_input_zero_point_0', 'distilbert.transformer.layer.2.attention.k_lin.module.zero_point', 'distilbert.transformer.layer.4.attention.q_lin.module._packed_params.dtype', 'distilbert.transformer.layer.4.attention.out_lin.module.scale', 'distilbert.transformer.layer.5.attention.out_lin.module._packed_params._packed_params', 'distilbert.transformer.layer.0.sa_layer_norm.module.weight', 'distilbert.transformer.layer.0.ffn.lin2_input_zero_point_0', 'distilbert.transformer.layer.5.attention.k_lin.module_input_zero_point_0', 'distilbert.transformer.layer.5.attention.v_lin.module_input_scale_0', 'distilbert.transformer.layer.0.attention.out_lin.module.scale', 'distilbert.transformer.layer.3.sa_layer_norm.module.weight', 'distilbert.transformer.layer.2.attention.k_lin.module._packed_params._packed_params', 'distilbert.transformer.layer.0.attention.q_lin.module.scale', 'distilbert.transformer.layer.4.output_layer_norm.module.bias', 'distilbert.transformer.layer.2.attention.v_lin.module_input_zero_point_0', 'distilbert.transformer.layer.1.attention.q_lin.module._packed_params.dtype', 'distilbert.transformer.layer.3.sa_layer_norm.module.bias', 'distilbert.transformer.layer.1.attention.v_lin.module.scale', 'distilbert.transformer.layer.1.sa_layer_norm.module.bias', 'distilbert.transformer.layer.1.ffn.lin1.scale', 'distilbert.transformer.layer.2.ffn.lin1_input_scale_0', 'distilbert.transformer.layer.4.output_layer_norm.module.weight', 'distilbert.transformer.layer.3.ffn.lin1.scale', 'distilbert.transformer.layer.3.ffn.lin1_input_zero_point_0', 'distilbert.embeddings.LayerNorm.module.bias', 'distilbert.transformer.layer.1.attention.q_lin.module._packed_params._packed_params', 'distilbert.transformer.layer.3.attention.v_lin.module._packed_params.dtype', 'distilbert.transformer.layer.0.attention.out_lin.module_input_scale_0', 'distilbert.transformer.layer.0.attention.q_lin.module_input_scale_0', 'distilbert.transformer.layer.2.attention.q_lin.module.zero_point', 'distilbert.transformer.layer.3.attention.out_lin.module_input_scale_0', 'distilbert.transformer.layer.4.attention.v_lin.module._packed_params._packed_params', 'distilbert.transformer.layer.2.ffn.lin1._packed_params.dtype', 'distilbert.transformer.layer.3.output_layer_norm.module.weight', 'distilbert.transformer.layer.2.ffn.lin1._packed_params._packed_params', 'distilbert.transformer.layer.1.attention.out_lin.module_input_zero_point_0', 'distilbert.transformer.layer.4.ffn.lin1.zero_point', 'distilbert.transformer.layer.4.attention.out_lin.module._packed_params._packed_params', 'distilbert.transformer.layer.0.ffn.lin2.scale', 'distilbert.transformer.layer.4.attention.v_lin.module_input_zero_point_0', 'distilbert.transformer.layer.3.attention.v_lin.module.zero_point', 'distilbert.transformer.layer.4.ffn.lin2.zero_point', 'distilbert.transformer.layer.0.ffn.lin1._packed_params._packed_params', 'distilbert.transformer.layer.1.attention.q_lin.module.zero_point', 'distilbert.transformer.layer.5.attention.v_lin.module._packed_params._packed_params', 'distilbert.transformer.layer.1.attention.v_lin.module._packed_params._packed_params', 'distilbert.embeddings.word_embeddings.module._packed_params.dtype', 'distilbert.transformer.layer.5.attention.v_lin.module_input_zero_point_0', 'distilbert.transformer.layer.4.attention.v_lin.module._packed_params.dtype', 'distilbert.transformer.layer.5.attention.q_lin.module.zero_point', 'distilbert.transformer.layer.5.attention.q_lin.module.scale', 'distilbert.transformer.layer.4.attention.q_lin.module.scale', 'distilbert.transformer.layer.2.ffn.lin2.scale', 'distilbert.transformer.layer.5.ffn.lin1._packed_params.dtype', 'distilbert.transformer.layer.3.attention.q_lin.module._packed_params.dtype', 'distilbert.transformer.layer.2.attention.out_lin.module._packed_params.dtype', 'distilbert.transformer.layer.1.sa_layer_norm.module.weight', 'distilbert.transformer.layer.2.attention.k_lin.module_input_zero_point_0', 'distilbert.embeddings.LayerNorm.module.weight', 'distilbert.transformer.layer.2.attention.k_lin.module._packed_params.dtype', 'distilbert.transformer.layer.1.attention.k_lin.module.scale', 'distilbert.transformer.layer.3.attention.q_lin.module.scale', 'pre_classifier.module_input_zero_point_0', 'distilbert.transformer.layer.3.ffn.lin2_input_zero_point_0', 'distilbert.transformer.layer.5.output_layer_norm.module.weight', 'classifier.module._packed_params._packed_params', 'distilbert.transformer.layer.5.sa_layer_norm.module.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/Colab Notebooks/advanced_deep_learning_group12/static_quantization and are newly initialized: ['distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'classifier.bias', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'classifier.weight', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'pre_classifier.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'pre_classifier.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.3.ffn.lin2.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of the model checkpoint at /content/drive/MyDrive/Colab Notebooks/advanced_deep_learning_group12/knowledge_distillation were not used when initializing DistilBertForSequenceClassification: ['best_configure']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","<ipython-input-41-1fb81ea182ae>:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  eval_distilbert_padded = torch.tensor(eval_distilbert_padded, dtype=torch.long).to(device)\n","<ipython-input-26-e8444779446b>:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  return {'input_ids': torch.tensor(self.tokenized_inputs[idx], dtype=torch.long),\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","<ipython-input-26-e8444779446b>:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  return {'input_ids': torch.tensor(self.tokenized_inputs[idx], dtype=torch.long),\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","<ipython-input-26-e8444779446b>:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  return {'input_ids': torch.tensor(self.tokenized_inputs[idx], dtype=torch.long),\n"]}]},{"cell_type":"code","source":["from tabulate import tabulate\n","\n","# Calculate metrics for model 1\n","model1_accuracy, model1_precision, model1_recall, model1_f1 = calculate_metrics(model1_predictions, true_labels)\n","\n","# Calculate metrics for model 2\n","model2_accuracy, model2_precision, model2_recall, model2_f1 = calculate_metrics(model2_predictions, true_labels)\n","\n","# Calculate metrics for model 3\n","model3_accuracy, model3_precision, model3_recall, model3_f1 = calculate_metrics(model3_predictions, true_labels)\n","\n","# Create a table to display the metrics\n","table = [[\"Model Compression\", \"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"],\n","         [\"Model 1 - Pruning\", model1_accuracy, model1_precision, model1_recall, model1_f1],\n","         [\"Model 2 - Quantized Model\", model2_accuracy, model2_precision, model2_recall, model2_f1],\n","         [\"Model 3 - Knowledge Distiliation\", model3_accuracy, model3_precision, model3_recall, model3_f1]]\n","\n","# Print the table\n","print(tabulate(table, headers=\"firstrow\"))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cNBxEhJSk_TS","executionInfo":{"status":"ok","timestamp":1687982572722,"user_tz":-180,"elapsed":388,"user":{"displayName":"Dana Tal","userId":"05782097349471177651"}},"outputId":"186b0e65-fdc5-4011-e887-9266e8414467"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["Model Compression                   Accuracy    Precision     Recall    F1 Score\n","--------------------------------  ----------  -----------  ---------  ----------\n","Model 1 - Pruning                  0.0670807    0.251578   0.0670807  0.0191864\n","Model 2 - Quantized Model          0.0608696    0.0037051  0.0608696  0.00698503\n","Model 3 - Knowledge Distiliation   0.768944     0.772281   0.768944   0.766613\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]}]}